name: Scheduled Scraper Job

on:
  schedule:
    - cron: '*/1 * * * *' # Runs every minute
    # - cron: '0 */16 * * * *' # Runs every 16 hours

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v2

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v1

    - name: Log in to GitHub Docker registry
      uses: docker/login-action@v1
      with:
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Build the Docker image
      run: docker build -t job-scraper .

    - name: Run parser.py scraper and save to database
      env:
        DB_HOST: ${{ secrets.DB_HOST }}
        DB_PORT: ${{ secrets.DB_PORT }}
        DB_USER: ${{ secrets.DB_USER }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        DB_NAME: ${{ secrets.DB_NAME }}
        EMAIL: ${{ secrets.EMAIL }}
        PASSWORD: ${{ secrets.PASSWORD }}
      run: docker run --env DB_HOST --env DB_PORT --env DB_USER --env DB_PASSWORD --env DB_NAME --env EMAIL --env PASSWORD job-scraper python scraper/parser.py

    - name: Run jobsearch_az.py scraper and save to database
      env:
        DB_HOST: ${{ secrets.DB_HOST }}
        DB_PORT: ${{ secrets.DB_PORT }}
        DB_USER: ${{ secrets.DB_USER }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        DB_NAME: ${{ secrets.DB_NAME }}
        EMAIL: ${{ secrets.EMAIL }}
        PASSWORD: ${{ secrets.PASSWORD }}
      run: docker run --env DB_HOST --env DB_PORT --env DB_USER --env DB_PASSWORD --env DB_NAME --env EMAIL --env PASSWORD job-scraper python scraper/jobsearch_az.py
